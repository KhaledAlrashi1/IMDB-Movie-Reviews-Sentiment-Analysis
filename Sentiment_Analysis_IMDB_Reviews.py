# -*- coding: utf-8 -*-
"""Sentiment_analysis_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W_iXUnHbDf3Z4TcYcpBbedxzOlaAEiDP

#### Libraries
"""

# from keras.datasets import imdb
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

import matplotlib.pyplot as plt

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense

import numpy as np

import pandas as pd

from keras.layers import Dropout
from keras.regularizers import l1, l2
from keras.optimizers import SGD, Adam, RMSprop, Adagrad

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV

"""#### 1. Data Collection
##### In this step, we use the keras library to load the IMDB dataset. The dataset contains movie reviews that are either positive (label = 1) or negative (label = 0).
"""

# from keras.datasets import imdb
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences


# Load the IMDB dataset.
# `num_words=10000` ensures that we only consider the top 10,000 most frequently occurring words in the training data.
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

"""#### 2. Data Exploration
###### Here we're just trying to understand the shape and structure of our dataset. It's useful to know how many samples we have and the nature of the samples.
"""

# Print the shape of training data and labels
print("Training data shape:", train_data.shape)
print("Training labels shape:", train_labels.shape)

"""#### Histogram to analyze the distribution of review lengths in the dataset to optimally choose a value that captures a high percentage (e.g., 90%) of the reviews without truncation."""

import matplotlib.pyplot as plt

# First, let's get the lengths of all the reviews in the training set
review_lengths = [len(review) for review in train_data]

# Plotting the histogram
plt.figure(figsize=(10, 5))
plt.hist(review_lengths, bins=50, color='blue', alpha=0.7)
plt.title('Distribution of Review Lengths')
plt.xlabel('Review Length')
plt.ylabel('Number of Reviews')
plt.grid(True, which="both", ls="--", c='0.65')
plt.tight_layout()
plt.show()

# Calculate some statistics to help decide on an optimal maxlen
mean_length = sum(review_lengths) / len(review_lengths)
median_length = sorted(review_lengths)[len(review_lengths) // 2]

mean_length, median_length

"""#### 3. Data Preprocessing
###### Data preprocessing is essential for ensuring that the data can be fed into a machine learning model. The IMDB dataset is already tokenized, which means each word is represented by a unique integer. However, reviews can be of different lengths, so we pad them to ensure they all have the same length.
"""

# Pad sequences to ensure each review has a uniform length.
# `maxlen=350` ensures each review has a length of 350,
# either by truncating longer reviews or padding shorter ones with zeros.

# We chose 350 as the maximum length since from the histogram, the mean is 238
# This way, we're capturing high percentage ofthe reviews without truncation
max_len = 350

train_data = pad_sequences(train_data, maxlen=max_len)
test_data = pad_sequences(test_data, maxlen=max_len)

"""#### 4. Model Building
###### This is where we define the architecture of our neural network.
"""

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense

# Initialize a sequential model
model = Sequential()

# Add an embedding layer to convert integer sequences into dense vectors of fixed size.
# The embedding layer will have a vocabulary size of 10,000 (due to `num_words=10000` in the data loading step)
# and output a 32-dimensional vector for each word.
model.add(Embedding(10000, 64, input_length=max_len))

# Add an LSTM layer, which is a type of recurrent neural network (RNN) layer.
# LSTM layers are good for sequence data like our movie reviews.
model.add(LSTM(64))

# Add a dense layer with a sigmoid activation function to produce a binary output
# (i.e., a value between 0 and 1, representing negative and positive sentiment respectively).
model.add(Dense(1, activation='sigmoid'))

# Compile the model, specifying the optimizer, loss function, and metrics for evaluation.
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])

# Train the model using the training data.
# We also specify a validation split, which means a portion (20% in this case) of the training data
# will be set aside to validate the model's performance during training.
history = model.fit(train_data, train_labels, epochs=10, batch_size=64, validation_split=0.2)

"""#### 5. Evaluation
###### After training, it's essential to evaluate the model's performance on unseen data to gauge its effectiveness.
"""

# Evaluate the model's performance on the test data.
loss, accuracy = model.evaluate(test_data, test_labels)

# Print the accuracy achieved on the test set.
print(f"Test Accuracy: {accuracy*100:.2f}%")

"""#### 6. Save the model & download it on your local machine"""

model.save("sentiment_analysis_model_lenmax_350.h5")

from google.colab import files
files.download("sentiment_analysis_model_lenmax_350.h5")

"""#### (Optional) Upload the model on Google Colab & load the model"""

# uploaded = files.upload()

# for fn in uploaded.keys():
#   print('User uploaded file "{name}" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))

# # After uploading, you can load your model back.
# from keras.models import load_model

# model = load_model("sentiment_analysis_model_lenmax_350.h5")

"""#### 7. Presentation
###### This is a simple step to showcase how the model performs on individual samples from the test set.
"""

import numpy as np

# Sample some test data to make predictions.
sample_data = test_data[:10]
predictions = model.predict(sample_data)

# For each prediction, determine if the sentiment is positive or negative and print the result.
for i, pred in enumerate(predictions):
    sentiment = "Positive" if pred > 0.5 else "Negative"
    print(f"Review {i+1}: {sentiment}")

